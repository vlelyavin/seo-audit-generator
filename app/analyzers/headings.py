"""Headings (H1) analyzer."""

from collections import Counter
from typing import Any, Dict, List

from ..models import AnalyzerResult, AuditIssue, PageData, SeverityLevel
from .base import BaseAnalyzer


class HeadingsAnalyzer(BaseAnalyzer):
    """Analyzer for H1 headings."""

    name = "headings"
    display_name = "–ó–∞–≥–æ–ª–æ–≤–∫–∏ H1"
    description = "–ó–∞–≥–æ–ª–æ–≤–æ–∫ H1 —î –≤–∞–∂–ª–∏–≤–∏–º –µ–ª–µ–º–µ–Ω—Ç–æ–º –¥–ª—è SEO. –ö–æ–∂–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ø–æ–≤–∏–Ω–Ω–∞ –º–∞—Ç–∏ –æ–¥–∏–Ω —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π H1."
    icon = "üìù"
    theory = """<strong>–ó–∞–≥–æ–ª–æ–≤–æ–∫ H1</strong> ‚Äî —Ü–µ –≥–æ–ª–æ–≤–Ω–∏–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤ HTML-—Ä–æ–∑–º—ñ—Ç—Ü—ñ. –ü–æ—à—É–∫–æ–≤—ñ —Å–∏—Å—Ç–µ–º–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å H1 –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è –æ—Å–Ω–æ–≤–Ω–æ—ó —Ç–µ–º–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏.

<strong>–ü—Ä–∞–≤–∏–ª–∞ –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—ó:</strong>
‚Ä¢ –ö–æ–∂–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ø–æ–≤–∏–Ω–Ω–∞ –º–∞—Ç–∏ <strong>—Ä—ñ–≤–Ω–æ –æ–¥–∏–Ω H1</strong>
‚Ä¢ H1 –ø–æ–≤–∏–Ω–µ–Ω –±—É—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–º –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–∞–π—Ç—É
‚Ä¢ –í–∫–ª—é—á–∞–π—Ç–µ –æ—Å–Ω–æ–≤–Ω–µ –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ –≤ H1
‚Ä¢ H1 –º–∞—î –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –∑–º—ñ—Å—Ç—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Ç–∞ –±—É—Ç–∏ —Å—Ö–æ–∂–∏–º –Ω–∞ Title

<strong>–ß–æ–º—É —Ü–µ –≤–∞–∂–ª–∏–≤–æ:</strong>
‚Ä¢ Google –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î H1 —è–∫ —Å–∏–≥–Ω–∞–ª —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ
‚Ä¢ –í—ñ–¥—Å—É—Ç–Ω—ñ—Å—Ç—å H1 —É—Å–∫–ª–∞–¥–Ω—é—î —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–µ–º–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
‚Ä¢ –ö—ñ–ª—å–∫–∞ H1 –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ —Ä–æ–∑–º–∏–≤–∞—é—Ç—å —Ç–µ–º–∞—Ç–∏—á–Ω–∏–π —Ñ–æ–∫—É—Å
‚Ä¢ –î—É–±–ª—ñ H1 –Ω–∞ —Ä—ñ–∑–Ω–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–∫–∞—Ö –º–æ–∂—É—Ç—å –ø—Ä–∏–∑–≤–µ—Å—Ç–∏ –¥–æ –∫–∞–Ω—ñ–±–∞–ª—ñ–∑–∞—Ü—ñ—ó –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤

<strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:</strong>
‚Ä¢ –°—Ç—Ä—É–∫—Ç—É—Ä—É–π—Ç–µ –∫–æ–Ω—Ç–µ–Ω—Ç —ñ—î—Ä–∞—Ä—Ö—ñ—á–Ω–æ: H1 ‚Üí H2 ‚Üí H3
‚Ä¢ –ù–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ H1 –¥–ª—è –ª–æ–≥–æ—Ç–∏–ø—É —á–∏ —Å–ª–æ–≥–∞–Ω—É
‚Ä¢ H1 –º–∞—î –±—É—Ç–∏ –≤–∏–¥–∏–º–∏–º –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ (–Ω–µ –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–º CSS)"""

    async def analyze(
        self,
        pages: Dict[str, PageData],
        base_url: str,
        **kwargs: Any
    ) -> AnalyzerResult:
        issues: List[AuditIssue] = []
        tables: List[Dict[str, Any]] = []

        # Collect H1 data
        all_h1s = {}
        missing_h1 = []
        multiple_h1 = []
        empty_h1 = []

        for url, page in pages.items():
            if page.status_code != 200:
                continue

            h1_tags = page.h1_tags

            if not h1_tags:
                missing_h1.append(url)
            elif len(h1_tags) > 1:
                multiple_h1.append((url, h1_tags))
                all_h1s[url] = h1_tags[0]  # Take first for duplicate check
            else:
                h1_text = h1_tags[0].strip()
                if not h1_text:
                    empty_h1.append(url)
                else:
                    all_h1s[url] = h1_text

        # Find duplicate H1s
        h1_counts = Counter(all_h1s.values())
        duplicate_h1s = {h1: count for h1, count in h1_counts.items() if count > 1}

        # Create issues
        if missing_h1:
            issues.append(self.create_issue(
                category="missing_h1",
                severity=SeverityLevel.ERROR,
                message=f"–í—ñ–¥—Å—É—Ç–Ω—ñ–π H1: {len(missing_h1)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details="–ó–∞–≥–æ–ª–æ–≤–æ–∫ H1 –¥–æ–ø–æ–º–∞–≥–∞—î –ø–æ—à—É–∫–æ–≤–∏–º —Å–∏—Å—Ç–µ–º–∞–º –∑—Ä–æ–∑—É–º—ñ—Ç–∏ —Ç–µ–º—É —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                affected_urls=missing_h1[:20],
                recommendation="–î–æ–¥–∞–π—Ç–µ –æ–¥–∏–Ω –∑–∞–≥–æ–ª–æ–≤–æ–∫ <h1> –Ω–∞ –∫–æ–∂–Ω—É —Å—Ç–æ—Ä—ñ–Ω–∫—É.",
                count=len(missing_h1),
            ))

        if multiple_h1:
            issues.append(self.create_issue(
                category="multiple_h1",
                severity=SeverityLevel.WARNING,
                message=f"–î–µ–∫—ñ–ª—å–∫–∞ H1: {len(multiple_h1)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details="–†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –º–∞—Ç–∏ –ª–∏—à–µ –æ–¥–∏–Ω H1 –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω—Ü—ñ –¥–ª—è —á—ñ—Ç–∫–æ—ó —Å—Ç—Ä—É–∫—Ç—É—Ä–∏.",
                affected_urls=[url for url, _ in multiple_h1[:20]],
                recommendation="–ó–∞–ª–∏—à—Ç–µ –ª–∏—à–µ –æ–¥–∏–Ω H1, —ñ–Ω—à—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∑–º—ñ–Ω—ñ—Ç—å –Ω–∞ H2-H6.",
                count=len(multiple_h1),
            ))

        if empty_h1:
            issues.append(self.create_issue(
                category="empty_h1",
                severity=SeverityLevel.ERROR,
                message=f"–ü–æ—Ä–æ–∂–Ω—ñ–π H1: {len(empty_h1)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details="H1 –±–µ–∑ —Ç–µ–∫—Å—Ç—É –Ω–µ –Ω–µ—Å–µ –∂–æ–¥–Ω–æ—ó SEO-—Ü—ñ–Ω–Ω–æ—Å—Ç—ñ.",
                affected_urls=empty_h1[:20],
                recommendation="–ó–∞–ø–æ–≤–Ω—ñ—Ç—å H1 —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º —ñ–∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏.",
                count=len(empty_h1),
            ))

        if duplicate_h1s:
            dup_urls = []
            for h1, count in duplicate_h1s.items():
                urls_with_h1 = [url for url, h in all_h1s.items() if h == h1]
                dup_urls.extend(urls_with_h1[:5])

            issues.append(self.create_issue(
                category="duplicate_h1",
                severity=SeverityLevel.WARNING,
                message=f"–î—É–±–ª—ñ H1: {len(duplicate_h1s)} –≥—Ä—É–ø –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤",
                details="–£–Ω—ñ–∫–∞–ª—å–Ω–∏–π H1 –¥–æ–ø–æ–º–∞–≥–∞—î —Ä–æ–∑—Ä—ñ–∑–Ω—è—Ç–∏ —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                affected_urls=dup_urls[:20],
                recommendation="–°—Ç–≤–æ—Ä—ñ—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π H1 –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                count=sum(duplicate_h1s.values()),
            ))

        # Create table with problematic pages
        table_data = []

        for url in missing_h1[:10]:
            table_data.append({
                "URL": url,
                "–ü—Ä–æ–±–ª–µ–º–∞": "–í—ñ–¥—Å—É—Ç–Ω—ñ–π H1",
                "H1": "-",
            })

        for url, h1_list in multiple_h1[:10]:
            table_data.append({
                "URL": url,
                "–ü—Ä–æ–±–ª–µ–º–∞": f"–î–µ–∫—ñ–ª—å–∫–∞ H1 ({len(h1_list)} —à—Ç.)",
                "H1": " | ".join(h1_list[:3]) + ("..." if len(h1_list) > 3 else ""),
            })

        for url in empty_h1[:10]:
            table_data.append({
                "URL": url,
                "–ü—Ä–æ–±–ª–µ–º–∞": "–ü–æ—Ä–æ–∂–Ω—ñ–π H1",
                "H1": "(–ø–æ—Ä–æ–∂–Ω—å–æ)",
            })

        if table_data:
            tables.append({
                "title": "–ü—Ä–æ–±–ª–µ–º–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏",
                "headers": ["URL", "–ü—Ä–æ–±–ª–µ–º–∞", "H1"],
                "rows": table_data,
            })

        # Summary
        total_pages = len([p for p in pages.values() if p.status_code == 200])
        ok_pages = total_pages - len(missing_h1) - len(multiple_h1) - len(empty_h1)

        summary_parts = []
        if missing_h1:
            summary_parts.append(f"–±–µ–∑ H1: {len(missing_h1)}")
        if multiple_h1:
            summary_parts.append(f"–¥–µ–∫—ñ–ª—å–∫–∞ H1: {len(multiple_h1)}")
        if duplicate_h1s:
            summary_parts.append(f"–¥—É–±–ª—ñ–≤ H1: {len(duplicate_h1s)}")

        if summary_parts:
            summary = f"–ó–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–±–ª–µ–º–∏: {', '.join(summary_parts)}"
        else:
            summary = f"–í—Å—ñ {total_pages} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –º–∞—é—Ç—å –∫–æ—Ä–µ–∫—Ç–Ω–∏–π H1"

        severity = self._determine_overall_severity(issues)

        return self.create_result(
            severity=severity,
            summary=summary,
            issues=issues,
            data={
                "total_pages": total_pages,
                "missing_h1": len(missing_h1),
                "multiple_h1": len(multiple_h1),
                "empty_h1": len(empty_h1),
                "duplicate_h1": len(duplicate_h1s),
                "ok_pages": ok_pages,
            },
            tables=tables,
        )
