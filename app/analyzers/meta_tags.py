"""Meta tags analyzer."""

from collections import Counter
from typing import Any, Dict, List

from ..config import settings
from ..models import AnalyzerResult, AuditIssue, PageData, SeverityLevel
from .base import BaseAnalyzer


class MetaTagsAnalyzer(BaseAnalyzer):
    """Analyzer for meta tags (title, description)."""

    name = "meta_tags"
    display_name = "–ú–µ—Ç–∞-—Ç–µ–≥–∏"
    description = "–ú–µ—Ç–∞-—Ç–µ–≥–∏ Title —Ç–∞ Description –≤–ø–ª–∏–≤–∞—é—Ç—å –Ω–∞ —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è —Ç–∞ –∫–ª—ñ–∫–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å —É –ø–æ—à—É–∫–æ–≤—ñ–π –≤–∏–¥–∞—á—ñ."
    icon = "üè∑Ô∏è"
    theory = """<strong>Title (–∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–æ—Ä—ñ–Ω–∫–∏)</strong> ‚Äî —Ü–µ HTML-—Ç–µ–≥, —è–∫–∏–π –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î—Ç—å—Å—è –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö –ø–æ—à—É–∫—É —è–∫ –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å–Ω—ñ–ø–µ—Ç—É. –í—ñ–Ω —î –æ–¥–Ω–∏–º –∑ –Ω–∞–π–≤–∞–∂–ª–∏–≤—ñ—à–∏—Ö —Ñ–∞–∫—Ç–æ—Ä—ñ–≤ —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è.

<strong>–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞:</strong> 50-60 —Å–∏–º–≤–æ–ª—ñ–≤. Google –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î –ø—Ä–∏–±–ª–∏–∑–Ω–æ 600 –ø—ñ–∫—Å–µ–ª—ñ–≤ (–±–ª–∏–∑—å–∫–æ 60 —Å–∏–º–≤–æ–ª—ñ–≤). –ó–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫—ñ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –≤–µ—Å—å –ø–æ—Ç–µ–Ω—Ü—ñ–∞–ª, –∑–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥—ñ ‚Äî –æ–±—Ä—ñ–∑–∞—é—Ç—å—Å—è.

<strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:</strong>
‚Ä¢ –ö–æ–∂–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ø–æ–≤–∏–Ω–Ω–∞ –º–∞—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π Title
‚Ä¢ –í–∫–ª—é—á–∞–π—Ç–µ –æ—Å–Ω–æ–≤–Ω–µ –∫–ª—é—á–æ–≤–µ —Å–ª–æ–≤–æ –Ω–∞ –ø–æ—á–∞—Ç–∫—É
‚Ä¢ –†–æ–±—ñ—Ç—å –∑–∞–≥–æ–ª–æ–≤–æ–∫ –ø—Ä–∏–≤–∞–±–ª–∏–≤–∏–º –¥–ª—è –∫–ª—ñ–∫—É

<strong>Description (–º–µ—Ç–∞-–æ–ø–∏—Å)</strong> ‚Äî HTML-—Ç–µ–≥, —è–∫–∏–π –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î—Ç—å—Å—è –ø—ñ–¥ –∑–∞–≥–æ–ª–æ–≤–∫–æ–º —É –ø–æ—à—É–∫–æ–≤—ñ–π –≤–∏–¥–∞—á—ñ. –ù–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞–ø—Ä—è–º—É –Ω–∞ —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è, –∞–ª–µ –≤–ø–ª–∏–≤–∞—î –Ω–∞ CTR (–∫–ª—ñ–∫–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å).

<strong>–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞:</strong> 150-160 —Å–∏–º–≤–æ–ª—ñ–≤. Google –≤—ñ–¥–æ–±—Ä–∞–∂–∞—î –ø—Ä–∏–±–ª–∏–∑–Ω–æ 920 –ø—ñ–∫—Å–µ–ª—ñ–≤ –¥–ª—è –¥–µ—Å–∫—Ç–æ–ø—É.

<strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:</strong>
‚Ä¢ –û–ø–∏—à—ñ—Ç—å –∑–º—ñ—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å—Ç–∏—Å–ª–æ —Ç–∞ —ñ–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ
‚Ä¢ –í–∫–ª—é—á—ñ—Ç—å –∑–∞–∫–ª–∏–∫ –¥–æ –¥—ñ—ó (CTA)
‚Ä¢ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –ø—Ä–∏—Ä–æ–¥–Ω–æ"""

    async def analyze(
        self,
        pages: Dict[str, PageData],
        base_url: str,
        **kwargs: Any
    ) -> AnalyzerResult:
        issues: List[AuditIssue] = []
        tables: List[Dict[str, Any]] = []

        # Collect all titles and descriptions
        titles = {}
        descriptions = {}
        missing_titles = []
        missing_descriptions = []
        short_titles = []
        long_titles = []
        short_descriptions = []
        long_descriptions = []

        for url, page in pages.items():
            if page.status_code != 200:
                continue

            # Check title
            if not page.title:
                missing_titles.append(url)
            else:
                titles[url] = page.title
                title_len = len(page.title)
                if title_len < settings.TITLE_MIN_LENGTH:
                    short_titles.append((url, page.title, title_len))
                elif title_len > settings.TITLE_MAX_LENGTH:
                    long_titles.append((url, page.title, title_len))

            # Check description
            if not page.meta_description:
                missing_descriptions.append(url)
            else:
                descriptions[url] = page.meta_description
                desc_len = len(page.meta_description)
                if desc_len < settings.DESCRIPTION_MIN_LENGTH:
                    short_descriptions.append((url, page.meta_description, desc_len))
                elif desc_len > settings.DESCRIPTION_MAX_LENGTH:
                    long_descriptions.append((url, page.meta_description, desc_len))

        # Find duplicates
        title_counts = Counter(titles.values())
        duplicate_titles = {title: count for title, count in title_counts.items() if count > 1}

        desc_counts = Counter(descriptions.values())
        duplicate_descriptions = {desc: count for desc, count in desc_counts.items() if count > 1}

        # Create issues for missing titles
        if missing_titles:
            issues.append(self.create_issue(
                category="missing_title",
                severity=SeverityLevel.ERROR,
                message=f"–í—ñ–¥—Å—É—Ç–Ω—ñ–π Title: {len(missing_titles)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details="Title —î –≤–∞–∂–ª–∏–≤–∏–º —Ñ–∞–∫—Ç–æ—Ä–æ–º —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è. –ö–æ–∂–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞ –ø–æ–≤–∏–Ω–Ω–∞ –º–∞—Ç–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π Title.",
                affected_urls=missing_titles[:20],
                recommendation="–î–æ–¥–∞–π—Ç–µ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —Ç–µ–≥ <title> –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                count=len(missing_titles),
            ))

        # Create issues for missing descriptions
        if missing_descriptions:
            issues.append(self.create_issue(
                category="missing_description",
                severity=SeverityLevel.ERROR,
                message=f"–í—ñ–¥—Å—É—Ç–Ω—ñ–π Description: {len(missing_descriptions)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details="Meta Description –≤–ø–ª–∏–≤–∞—î –Ω–∞ –∫–ª—ñ–∫–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å —É –ø–æ—à—É–∫–æ–≤—ñ–π –≤–∏–¥–∞—á—ñ.",
                affected_urls=missing_descriptions[:20],
                recommendation="–î–æ–¥–∞–π—Ç–µ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –º–µ—Ç–∞-—Ç–µ–≥ description –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                count=len(missing_descriptions),
            ))

        # Create issues for short titles
        if short_titles:
            issues.append(self.create_issue(
                category="short_title",
                severity=SeverityLevel.WARNING,
                message=f"–ó–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π Title: {len(short_titles)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details=f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ Title: {settings.TITLE_MIN_LENGTH}-{settings.TITLE_MAX_LENGTH} —Å–∏–º–≤–æ–ª—ñ–≤.",
                affected_urls=[url for url, _, _ in short_titles[:20]],
                recommendation="–†–æ–∑—à–∏—Ä—Ç–µ Title, –≤–∫–ª—é—á–∏–≤—à–∏ –±—ñ–ª—å—à–µ –∫–ª—é—á–æ–≤–∏—Ö —Å–ª—ñ–≤.",
                count=len(short_titles),
            ))

        # Create issues for long titles
        if long_titles:
            issues.append(self.create_issue(
                category="long_title",
                severity=SeverityLevel.WARNING,
                message=f"–ó–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π Title: {len(long_titles)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details=f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ Title: {settings.TITLE_MIN_LENGTH}-{settings.TITLE_MAX_LENGTH} —Å–∏–º–≤–æ–ª—ñ–≤. –î–æ–≤—à–∏–π Title –±—É–¥–µ –æ–±—Ä—ñ–∑–∞–Ω–∏–π —É –≤–∏–¥–∞—á—ñ.",
                affected_urls=[url for url, _, _ in long_titles[:20]],
                recommendation="–°–∫–æ—Ä–æ—Ç—ñ—Ç—å Title –¥–æ 60 —Å–∏–º–≤–æ–ª—ñ–≤.",
                count=len(long_titles),
            ))

        # Create issues for short descriptions
        if short_descriptions:
            issues.append(self.create_issue(
                category="short_description",
                severity=SeverityLevel.WARNING,
                message=f"–ó–∞–Ω–∞–¥—Ç–æ –∫–æ—Ä–æ—Ç–∫–∏–π Description: {len(short_descriptions)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details=f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ Description: {settings.DESCRIPTION_MIN_LENGTH}-{settings.DESCRIPTION_MAX_LENGTH} —Å–∏–º–≤–æ–ª—ñ–≤.",
                affected_urls=[url for url, _, _ in short_descriptions[:20]],
                recommendation="–†–æ–∑—à–∏—Ä—Ç–µ Description, –æ–ø–∏—Å–∞–≤—à–∏ –≤–º—ñ—Å—Ç —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –¥–µ—Ç–∞–ª—å–Ω—ñ—à–µ.",
                count=len(short_descriptions),
            ))

        # Create issues for long descriptions
        if long_descriptions:
            issues.append(self.create_issue(
                category="long_description",
                severity=SeverityLevel.WARNING,
                message=f"–ó–∞–Ω–∞–¥—Ç–æ –¥–æ–≤–≥–∏–π Description: {len(long_descriptions)} —Å—Ç–æ—Ä—ñ–Ω–æ–∫",
                details=f"–û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ Description: {settings.DESCRIPTION_MIN_LENGTH}-{settings.DESCRIPTION_MAX_LENGTH} —Å–∏–º–≤–æ–ª—ñ–≤.",
                affected_urls=[url for url, _, _ in long_descriptions[:20]],
                recommendation="–°–∫–æ—Ä–æ—Ç—ñ—Ç—å Description –¥–æ 160 —Å–∏–º–≤–æ–ª—ñ–≤.",
                count=len(long_descriptions),
            ))

        # Create issues for duplicate titles
        if duplicate_titles:
            dup_urls = []
            for title, count in duplicate_titles.items():
                urls_with_title = [url for url, t in titles.items() if t == title]
                dup_urls.extend(urls_with_title[:5])

            issues.append(self.create_issue(
                category="duplicate_title",
                severity=SeverityLevel.ERROR,
                message=f"–î—É–±–ª—ñ Title: {len(duplicate_titles)} –≥—Ä—É–ø –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤",
                details="–î—É–±–ª—ñ Title —É—Å–∫–ª–∞–¥–Ω—é—é—Ç—å —Ä–æ–∑—É–º—ñ–Ω–Ω—è –ø–æ—à—É–∫–æ–≤–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏ —É–Ω—ñ–∫–∞–ª—å–Ω–æ—Å—Ç—ñ –∫–æ–Ω—Ç–µ–Ω—Ç—É.",
                affected_urls=dup_urls[:20],
                recommendation="–°—Ç–≤–æ—Ä—ñ—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π Title –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                count=sum(duplicate_titles.values()),
            ))

        # Create issues for duplicate descriptions
        if duplicate_descriptions:
            dup_urls = []
            for desc, count in duplicate_descriptions.items():
                urls_with_desc = [url for url, d in descriptions.items() if d == desc]
                dup_urls.extend(urls_with_desc[:5])

            issues.append(self.create_issue(
                category="duplicate_description",
                severity=SeverityLevel.WARNING,
                message=f"–î—É–±–ª—ñ Description: {len(duplicate_descriptions)} –≥—Ä—É–ø –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤",
                details="–£–Ω—ñ–∫–∞–ª—å–Ω–∏–π Description –ø—ñ–¥–≤–∏—â—É—î –∫–ª—ñ–∫–∞–±–µ–ª—å–Ω—ñ—Å—Ç—å —É –≤–∏–¥–∞—á—ñ.",
                affected_urls=dup_urls[:20],
                recommendation="–°—Ç–≤–æ—Ä—ñ—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π Description –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏.",
                count=sum(duplicate_descriptions.values()),
            ))

        # Create table with problematic pages
        if missing_titles or missing_descriptions or short_titles or long_titles:
            table_data = []
            seen_urls = set()

            for url in missing_titles[:10]:
                if url not in seen_urls:
                    table_data.append({
                        "URL": url,
                        "–ü—Ä–æ–±–ª–µ–º–∞": "–í—ñ–¥—Å—É—Ç–Ω—ñ–π Title",
                        "Title": "-",
                        "Description": pages[url].meta_description[:50] + "..." if pages[url].meta_description else "-",
                    })
                    seen_urls.add(url)

            for url in missing_descriptions[:10]:
                if url not in seen_urls:
                    table_data.append({
                        "URL": url,
                        "–ü—Ä–æ–±–ª–µ–º–∞": "–í—ñ–¥—Å—É—Ç–Ω—ñ–π Description",
                        "Title": pages[url].title[:50] + "..." if pages[url].title else "-",
                        "Description": "-",
                    })
                    seen_urls.add(url)

            if table_data:
                tables.append({
                    "title": "–ü—Ä–æ–±–ª–µ–º–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏",
                    "headers": ["URL", "–ü—Ä–æ–±–ª–µ–º–∞", "Title", "Description"],
                    "rows": table_data,
                })

        # Calculate summary
        total_pages = len([p for p in pages.values() if p.status_code == 200])
        ok_titles = total_pages - len(missing_titles) - len(short_titles) - len(long_titles)
        ok_descriptions = total_pages - len(missing_descriptions) - len(short_descriptions) - len(long_descriptions)

        summary_parts = []
        if missing_titles or missing_descriptions:
            summary_parts.append(f"–í—ñ–¥—Å—É—Ç–Ω—ñ –º–µ—Ç–∞-—Ç–µ–≥–∏: {len(missing_titles)} Title, {len(missing_descriptions)} Description")
        if duplicate_titles or duplicate_descriptions:
            summary_parts.append(f"–î—É–±–ª—ñ–∫–∞—Ç–∏: {len(duplicate_titles)} Title, {len(duplicate_descriptions)} Description")
        if not summary_parts:
            summary_parts.append(f"–í—Å—ñ {total_pages} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –º–∞—é—Ç—å –∫–æ—Ä–µ–∫—Ç–Ω—ñ –º–µ—Ç–∞-—Ç–µ–≥–∏")

        severity = self._determine_overall_severity(issues)

        return self.create_result(
            severity=severity,
            summary=". ".join(summary_parts),
            issues=issues,
            data={
                "total_pages": total_pages,
                "missing_titles": len(missing_titles),
                "missing_descriptions": len(missing_descriptions),
                "duplicate_titles": len(duplicate_titles),
                "duplicate_descriptions": len(duplicate_descriptions),
                "ok_titles": ok_titles,
                "ok_descriptions": ok_descriptions,
            },
            tables=tables,
        )
