"""Robots and indexing analyzer."""

import re
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urljoin, urlparse

from ..crawler import fetch_url_content
from ..models import AnalyzerResult, AuditIssue, PageData, RobotsTxtData, SitemapData, SeverityLevel
from .base import BaseAnalyzer


class RobotsAnalyzer(BaseAnalyzer):
    """Analyzer for robots.txt, sitemap.xml, and indexing issues."""

    name = "robots"
    display_name = "–Ü–Ω–¥–µ–∫—Å–∞—Ü—ñ—è"
    description = "–ü—Ä–∞–≤–∏–ª—å–Ω–µ –Ω–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è robots.txt —Ç–∞ sitemap.xml –¥–æ–ø–æ–º–∞–≥–∞—î –ø–æ—à—É–∫–æ–≤–∏–º —Å–∏—Å—Ç–µ–º–∞–º –µ—Ñ–µ–∫—Ç–∏–≤–Ω–æ —ñ–Ω–¥–µ–∫—Å—É–≤–∞—Ç–∏ —Å–∞–π—Ç."
    icon = "ü§ñ"
    theory = """<strong>Robots.txt</strong> ‚Äî —Ñ–∞–π–ª, —â–æ –∫–µ—Ä—É—î –¥–æ—Å—Ç—É–ø–æ–º –ø–æ—à—É–∫–æ–≤–∏—Ö —Ä–æ–±–æ—Ç—ñ–≤ –¥–æ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ —Å–∞–π—Ç—É.

<strong>–û—Å–Ω–æ–≤–Ω—ñ –¥–∏—Ä–µ–∫—Ç–∏–≤–∏ robots.txt:</strong>
‚Ä¢ <code>User-agent: *</code> ‚Äî –ø—Ä–∞–≤–∏–ª–∞ –¥–ª—è –≤—Å—ñ—Ö —Ä–æ–±–æ—Ç—ñ–≤
‚Ä¢ <code>Disallow: /admin/</code> ‚Äî –∑–∞–±–æ—Ä–æ–Ω–∞ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó —à–ª—è—Ö—É
‚Ä¢ <code>Allow: /</code> ‚Äî –¥–æ–∑–≤—ñ–ª —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó
‚Ä¢ <code>Sitemap: URL</code> ‚Äî –ø–æ—Å–∏–ª–∞–Ω–Ω—è –Ω–∞ –∫–∞—Ä—Ç—É —Å–∞–π—Ç—É

<strong>Sitemap.xml</strong> ‚Äî XML-—Ñ–∞–π–ª –∑—ñ —Å–ø–∏—Å–∫–æ–º –≤—Å—ñ—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –¥–ª—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó.

<strong>–í–∞–∂–ª–∏–≤—ñ—Å—Ç—å sitemap:</strong>
‚Ä¢ –î–æ–ø–æ–º–∞–≥–∞—î –ø–æ—à—É–∫–æ–≤–∏–º —Ä–æ–±–æ—Ç–∞–º –∑–Ω–∞–π—Ç–∏ –≤—Å—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏
‚Ä¢ –ü–æ–∫–∞–∑—É—î –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç —Ç–∞ —á–∞—Å—Ç–æ—Ç—É –æ–Ω–æ–≤–ª–µ–Ω–Ω—è
‚Ä¢ –ü—Ä–∏—Å–∫–æ—Ä—é—î —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é –Ω–æ–≤–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫

<strong>Canonical —Ç–∞ Noindex:</strong>
‚Ä¢ <code>rel="canonical"</code> ‚Äî –≤–∫–∞–∑—É—î –æ—Å–Ω–æ–≤–Ω—É –≤–µ—Ä—Å—ñ—é —Å—Ç–æ—Ä—ñ–Ω–∫–∏ (—É–Ω–∏–∫–Ω–µ–Ω–Ω—è –¥—É–±–ª—ñ–≤)
‚Ä¢ <code>&lt;meta name="robots" content="noindex"&gt;</code> ‚Äî –∑–∞–±–æ—Ä–æ–Ω—è—î —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—é —Å—Ç–æ—Ä—ñ–Ω–∫–∏

<strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:</strong>
‚Ä¢ –ó–∞–≤–∂–¥–∏ –º–∞–π—Ç–µ robots.txt —Ç–∞ sitemap.xml
‚Ä¢ –í–∫–∞–∂—ñ—Ç—å sitemap —É robots.txt
‚Ä¢ –ù–µ –±–ª–æ–∫—É–π—Ç–µ CSS/JS —Ñ–∞–π–ª–∏
‚Ä¢ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ canonical –¥–ª—è –¥—É–±–ª—é—é—á–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫
‚Ä¢ –ü–æ–¥–∞–π—Ç–µ sitemap —É Google Search Console"""

    async def analyze(
        self,
        pages: Dict[str, PageData],
        base_url: str,
        **kwargs: Any
    ) -> AnalyzerResult:
        issues: List[AuditIssue] = []
        tables: List[Dict[str, Any]] = []

        # Check robots.txt
        robots_url = urljoin(base_url, "/robots.txt")
        robots_data = await self._analyze_robots_txt(robots_url)

        # Check sitemap.xml (support for multiple sitemaps and sitemap index)
        sitemap_urls_to_check = robots_data.sitemaps if robots_data.sitemaps else [urljoin(base_url, "/sitemap.xml")]
        sitemap_data, sitemap_urls_set, sitemap_lastmod = await self._analyze_all_sitemaps(sitemap_urls_to_check, base_url)

        # Check noindex pages
        noindex_pages = []
        canonical_issues = []

        for url, page in pages.items():
            if page.status_code != 200:
                continue

            # Check noindex
            if page.has_noindex:
                noindex_pages.append(url)

            # Check canonical
            if page.canonical:
                # Canonical should point to a valid URL
                if page.canonical != url:
                    # Different canonical - could be intentional
                    if page.canonical not in pages:
                        canonical_issues.append({
                            'url': url,
                            'canonical': page.canonical,
                            'issue': 'Canonical –≤–∫–∞–∑—É—î –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫—É –ø–æ–∑–∞ —Å–∞–π—Ç–æ–º –∞–±–æ –Ω–µ—ñ—Å–Ω—É—é—á—É',
                        })

        # Create issues for robots.txt
        if not robots_data.exists:
            issues.append(self.create_issue(
                category="no_robots_txt",
                severity=SeverityLevel.WARNING,
                message="–§–∞–π–ª robots.txt –≤—ñ–¥—Å—É—Ç–Ω—ñ–π",
                details="robots.txt –¥–æ–ø–æ–º–∞–≥–∞—î –∫–æ–Ω—Ç—Ä–æ–ª—é–≤–∞—Ç–∏, —è–∫—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —ñ–Ω–¥–µ–∫—Å—É—é—Ç—å—Å—è –ø–æ—à—É–∫–æ–≤–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏.",
                recommendation="–°—Ç–≤–æ—Ä—ñ—Ç—å —Ñ–∞–π–ª robots.txt —É –∫–æ—Ä–µ–Ω—ñ —Å–∞–π—Ç—É.",
            ))
        else:
            if robots_data.errors:
                issues.append(self.create_issue(
                    category="robots_txt_errors",
                    severity=SeverityLevel.WARNING,
                    message=f"–ü–æ–º–∏–ª–∫–∏ —É robots.txt: {len(robots_data.errors)} —à—Ç.",
                    details="; ".join(robots_data.errors[:5]),
                    recommendation="–í–∏–ø—Ä–∞–≤—Ç–µ —Å–∏–Ω—Ç–∞–∫—Å–∏—á–Ω—ñ –ø–æ–º–∏–ª–∫–∏ —É robots.txt.",
                    count=len(robots_data.errors),
                ))

            if not robots_data.sitemaps:
                issues.append(self.create_issue(
                    category="no_sitemap_in_robots",
                    severity=SeverityLevel.INFO,
                    message="Sitemap –Ω–µ –≤–∫–∞–∑–∞–Ω–æ —É robots.txt",
                    details="–†–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –≤–∫–∞–∑–∞—Ç–∏ —à–ª—è—Ö –¥–æ sitemap —É robots.txt.",
                    recommendation="–î–æ–¥–∞–π—Ç–µ —Ä—è–¥–æ–∫ 'Sitemap: https://yoursite.com/sitemap.xml' —É robots.txt.",
                ))

        # Create issues for sitemap
        if not sitemap_data.exists:
            issues.append(self.create_issue(
                category="no_sitemap",
                severity=SeverityLevel.ERROR,
                message="–§–∞–π–ª sitemap.xml –≤—ñ–¥—Å—É—Ç–Ω—ñ–π",
                details="Sitemap –¥–æ–ø–æ–º–∞–≥–∞—î –ø–æ—à—É–∫–æ–≤–∏–º —Å–∏—Å—Ç–µ–º–∞–º –∑–Ω–∞—Ö–æ–¥–∏—Ç–∏ –≤—Å—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ —Å–∞–π—Ç—É.",
                recommendation="–°—Ç–≤–æ—Ä—ñ—Ç—å sitemap.xml —Ç–∞ –¥–æ–¥–∞–π—Ç–µ –π–æ–≥–æ —É robots.txt —Ç–∞ Google Search Console.",
            ))
        else:
            if sitemap_data.errors:
                issues.append(self.create_issue(
                    category="sitemap_errors",
                    severity=SeverityLevel.WARNING,
                    message=f"–ü—Ä–æ–±–ª–µ–º–∏ –∑ sitemap: {len(sitemap_data.errors)} —à—Ç.",
                    details="; ".join(sitemap_data.errors[:5]),
                    recommendation="–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ –≤–∞–ª—ñ–¥–Ω—ñ—Å—Ç—å sitemap.xml.",
                    count=len(sitemap_data.errors),
                ))

            if sitemap_data.urls_count == 0:
                issues.append(self.create_issue(
                    category="empty_sitemap",
                    severity=SeverityLevel.WARNING,
                    message="Sitemap –ø–æ—Ä–æ–∂–Ω—ñ–π",
                    details="Sitemap –Ω–µ –º—ñ—Å—Ç–∏—Ç—å URL-–∞–¥—Ä–µ—Å.",
                    recommendation="–î–æ–¥–∞–π—Ç–µ URL-–∞–¥—Ä–µ—Å–∏ —Å—Ç–æ—Ä—ñ–Ω–æ–∫ —É sitemap.xml.",
                ))

            # Compare sitemap URLs with crawled pages
            if sitemap_urls_set:
                crawled_urls = set(url for url, page in pages.items() if page.status_code == 200)

                # URLs in sitemap but not found/crawled
                sitemap_not_crawled = sitemap_urls_set - crawled_urls
                # Filter to same domain only
                base_domain = urlparse(base_url).netloc
                sitemap_not_crawled = {url for url in sitemap_not_crawled if urlparse(url).netloc == base_domain}

                # URLs crawled but not in sitemap
                crawled_not_in_sitemap = crawled_urls - sitemap_urls_set
                # Filter to same domain only
                crawled_not_in_sitemap = {url for url in crawled_not_in_sitemap if urlparse(url).netloc == base_domain}

                if sitemap_not_crawled:
                    issues.append(self.create_issue(
                        category="sitemap_urls_not_found",
                        severity=SeverityLevel.WARNING,
                        message=f"URL —É sitemap –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –Ω–∞ —Å–∞–π—Ç—ñ: {len(sitemap_not_crawled)} —à—Ç.",
                        details="–¶—ñ URL –≤–∫–∞–∑–∞–Ω—ñ –≤ sitemap, –∞–ª–µ –Ω–µ –±—É–ª–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ –∫—Ä–∞—É–ª–µ—Ä–æ–º –∞–±–æ –ø–æ–≤–µ—Ä—Ç–∞—é—Ç—å –ø–æ–º–∏–ª–∫—É.",
                        affected_urls=list(sitemap_not_crawled)[:20],
                        recommendation="–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Ü—ñ URL —Ç–∞ –≤–∏–¥–∞–ª—ñ—Ç—å –Ω–µ—ñ—Å–Ω—É—é—á—ñ –∑ sitemap.",
                        count=len(sitemap_not_crawled),
                    ))

                if crawled_not_in_sitemap and len(crawled_not_in_sitemap) > 5:
                    issues.append(self.create_issue(
                        category="pages_not_in_sitemap",
                        severity=SeverityLevel.INFO,
                        message=f"–°—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ –≤–∫–ª—é—á–µ–Ω—ñ –≤ sitemap: {len(crawled_not_in_sitemap)} —à—Ç.",
                        details="–¶—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –∑–Ω–∞–π–¥–µ–Ω—ñ –Ω–∞ —Å–∞–π—Ç—ñ, –∞–ª–µ –≤—ñ–¥—Å—É—Ç–Ω—ñ –≤ sitemap.xml.",
                        affected_urls=list(crawled_not_in_sitemap)[:20],
                        recommendation="–î–æ–¥–∞–π—Ç–µ –≤–∞–∂–ª–∏–≤—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –≤ sitemap –¥–ª—è –∫—Ä–∞—â–æ—ó —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó.",
                        count=len(crawled_not_in_sitemap),
                    ))

            # Check lastmod dates
            if sitemap_lastmod:
                old_lastmod = []
                six_months_ago = datetime.now() - timedelta(days=180)

                for url, lastmod_str in sitemap_lastmod.items():
                    try:
                        # Parse various date formats
                        lastmod_date = None
                        for fmt in ['%Y-%m-%d', '%Y-%m-%dT%H:%M:%S', '%Y-%m-%dT%H:%M:%S%z', '%Y-%m-%d %H:%M:%S']:
                            try:
                                lastmod_date = datetime.strptime(lastmod_str[:19], fmt[:len(lastmod_str[:19])])
                                break
                            except ValueError:
                                continue

                        if lastmod_date and lastmod_date < six_months_ago:
                            old_lastmod.append((url, lastmod_str))
                    except Exception:
                        pass

                if old_lastmod and len(old_lastmod) > len(sitemap_lastmod) * 0.5:
                    issues.append(self.create_issue(
                        category="sitemap_old_lastmod",
                        severity=SeverityLevel.INFO,
                        message=f"–ó–∞—Å—Ç–∞—Ä—ñ–ª—ñ –¥–∞—Ç–∏ lastmod —É sitemap: {len(old_lastmod)} URL",
                        details="–ë—ñ–ª—å—à–µ –ø–æ–ª–æ–≤–∏–Ω–∏ URL –º–∞—é—Ç—å lastmod —Å—Ç–∞—Ä—à–µ 6 –º—ñ—Å—è—Ü—ñ–≤. –û–Ω–æ–≤—ñ—Ç—å –¥–∞—Ç–∏ –∞–±–æ –∫–æ–Ω—Ç–µ–Ω—Ç.",
                        affected_urls=[url for url, _ in old_lastmod[:10]],
                        recommendation="–û–Ω–æ–≤—ñ—Ç—å lastmod –¥–∞—Ç–∏ –≤ sitemap –ø—Ä–∏ –∑–º—ñ–Ω—ñ –∫–æ–Ω—Ç–µ–Ω—Ç—É —Å—Ç–æ—Ä—ñ–Ω–æ–∫.",
                    ))

        # Create issues for noindex pages
        if noindex_pages:
            issues.append(self.create_issue(
                category="noindex_pages",
                severity=SeverityLevel.INFO,
                message=f"–°—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ noindex: {len(noindex_pages)} —à—Ç.",
                details="–¶—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏ –Ω–µ –±—É–¥—É—Ç—å —ñ–Ω–¥–µ–∫—Å—É–≤–∞—Ç–∏—Å—è –ø–æ—à—É–∫–æ–≤–∏–º–∏ —Å–∏—Å—Ç–µ–º–∞–º–∏.",
                affected_urls=noindex_pages[:20],
                recommendation="–ü–µ—Ä–µ–∫–æ–Ω–∞–π—Ç–µ—Å—è, —â–æ noindex –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –Ω–∞–≤–º–∏—Å–Ω–æ –¥–ª—è —Ü–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫.",
                count=len(noindex_pages),
            ))

        # Create issues for canonical
        if canonical_issues:
            issues.append(self.create_issue(
                category="canonical_issues",
                severity=SeverityLevel.WARNING,
                message=f"–ü—Ä–æ–±–ª–µ–º–∏ –∑ canonical: {len(canonical_issues)} —à—Ç.",
                details="Canonical —Ç–µ–≥–∏ –≤–∫–∞–∑—É—é—Ç—å –Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∏, —è–∫—ñ –Ω–µ —ñ—Å–Ω—É—é—Ç—å –∞–±–æ –∑–Ω–∞—Ö–æ–¥—è—Ç—å—Å—è –ø–æ–∑–∞ —Å–∞–π—Ç–æ–º.",
                affected_urls=[c['url'] for c in canonical_issues[:20]],
                recommendation="–ü–µ—Ä–µ–≤—ñ—Ä—Ç–µ —Ç–∞ –≤–∏–ø—Ä–∞–≤—Ç–µ canonical —Ç–µ–≥–∏.",
                count=len(canonical_issues),
            ))

        # Create table with indexing status
        table_data = []

        table_data.append({
            "–ï–ª–µ–º–µ–Ω—Ç": "robots.txt",
            "–°—Ç–∞—Ç—É—Å": "‚úì –Ñ" if robots_data.exists else "‚úó –í—ñ–¥—Å—É—Ç–Ω—ñ–π",
            "–î–µ—Ç–∞–ª—ñ": f"Sitemap: {len(robots_data.sitemaps)}, Disallow: {len(robots_data.disallowed_paths)}" if robots_data.exists else "-",
        })

        table_data.append({
            "–ï–ª–µ–º–µ–Ω—Ç": "sitemap.xml",
            "–°—Ç–∞—Ç—É—Å": "‚úì –Ñ" if sitemap_data.exists else "‚úó –í—ñ–¥—Å—É—Ç–Ω—ñ–π",
            "–î–µ—Ç–∞–ª—ñ": f"URL: {sitemap_data.urls_count}, —Ñ–∞–π–ª—ñ–≤: {sitemap_data.sitemap_count}" if sitemap_data.exists else "-",
        })

        table_data.append({
            "–ï–ª–µ–º–µ–Ω—Ç": "noindex —Å—Ç–æ—Ä—ñ–Ω–∫–∏",
            "–°—Ç–∞—Ç—É—Å": f"{len(noindex_pages)} —à—Ç." if noindex_pages else "0",
            "–î–µ—Ç–∞–ª—ñ": noindex_pages[0][:50] + "..." if noindex_pages else "–ù–µ–º–∞—î",
        })

        tables.append({
            "title": "–°—Ç–∞—Ç—É—Å —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó",
            "headers": ["–ï–ª–µ–º–µ–Ω—Ç", "–°—Ç–∞—Ç—É—Å", "–î–µ—Ç–∞–ª—ñ"],
            "rows": table_data,
        })

        # Summary
        if not issues:
            summary = "–ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—ó –≤ –ø–æ—Ä—è–¥–∫—É"
        else:
            error_count = sum(1 for i in issues if i.severity == SeverityLevel.ERROR)
            warning_count = sum(1 for i in issues if i.severity == SeverityLevel.WARNING)
            parts = []
            if error_count:
                parts.append(f"–ø–æ–º–∏–ª–æ–∫: {error_count}")
            if warning_count:
                parts.append(f"–ø–æ–ø–µ—Ä–µ–¥–∂–µ–Ω—å: {warning_count}")
            summary = f"–ü—Ä–æ–±–ª–µ–º–∏ –∑ —ñ–Ω–¥–µ–∫—Å–∞—Ü—ñ—î—é: {', '.join(parts)}"

        severity = self._determine_overall_severity(issues)

        # Calculate sitemap comparison stats
        crawled_urls = set(url for url, page in pages.items() if page.status_code == 200)
        base_domain = urlparse(base_url).netloc
        sitemap_not_crawled_count = len({url for url in sitemap_urls_set - crawled_urls if urlparse(url).netloc == base_domain}) if sitemap_urls_set else 0
        crawled_not_in_sitemap_count = len({url for url in crawled_urls - sitemap_urls_set if urlparse(url).netloc == base_domain}) if sitemap_urls_set else 0

        return self.create_result(
            severity=severity,
            summary=summary,
            issues=issues,
            data={
                "robots_txt": robots_data.model_dump(),
                "sitemap": sitemap_data.model_dump(),
                "noindex_pages": len(noindex_pages),
                "canonical_issues": len(canonical_issues),
                "sitemap_urls_count": len(sitemap_urls_set),
                "sitemap_not_crawled": sitemap_not_crawled_count,
                "crawled_not_in_sitemap": crawled_not_in_sitemap_count,
            },
            tables=tables,
        )

    async def _analyze_robots_txt(self, url: str) -> RobotsTxtData:
        """Analyze robots.txt file."""
        status, content = await fetch_url_content(url)

        if status != 200 or not content:
            return RobotsTxtData(exists=False)

        sitemaps = []
        disallowed = []
        errors = []

        lines = content.split('\n')
        for i, line in enumerate(lines, 1):
            line = line.strip()

            # Skip comments and empty lines
            if not line or line.startswith('#'):
                continue

            # Parse directives
            if ':' in line:
                directive, _, value = line.partition(':')
                directive = directive.strip().lower()
                value = value.strip()

                if directive == 'sitemap':
                    sitemaps.append(value)
                elif directive == 'disallow':
                    if value:
                        disallowed.append(value)
                elif directive not in ['user-agent', 'allow', 'crawl-delay', 'host']:
                    errors.append(f"–†—è–¥–æ–∫ {i}: –Ω–µ–≤—ñ–¥–æ–º–∞ –¥–∏—Ä–µ–∫—Ç–∏–≤–∞ '{directive}'")
            else:
                if line and not line.startswith('#'):
                    errors.append(f"–†—è–¥–æ–∫ {i}: –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–∏–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å")

        return RobotsTxtData(
            exists=True,
            content=content,
            sitemaps=sitemaps,
            disallowed_paths=disallowed,
            errors=errors[:10],
        )

    async def _analyze_all_sitemaps(
        self,
        sitemap_urls: List[str],
        base_url: str
    ) -> Tuple[SitemapData, Set[str], Dict[str, str]]:
        """
        Analyze all sitemaps including sitemap index files.

        Returns:
            Tuple of (SitemapData, set of all URLs, dict of URL -> lastmod)
        """
        all_urls: Set[str] = set()
        all_lastmod: Dict[str, str] = {}
        all_errors: List[str] = []
        sitemap_count = 0
        total_urls = 0
        exists = False

        async def parse_sitemap(url: str, depth: int = 0) -> None:
            nonlocal sitemap_count, total_urls, exists

            if depth > 2:  # Prevent infinite recursion
                return

            status, content = await fetch_url_content(url)

            if status != 200 or not content:
                if depth == 0:
                    all_errors.append(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏: {url}")
                return

            exists = True
            sitemap_count += 1

            try:
                # Check if this is a sitemap index
                if '<sitemapindex' in content:
                    # Extract sitemap URLs from index
                    loc_matches = re.findall(r'<loc>([^<]+)</loc>', content)
                    for sitemap_url in loc_matches[:50]:  # Limit to 50 sitemaps
                        await parse_sitemap(sitemap_url.strip(), depth + 1)
                else:
                    # Regular sitemap - extract URLs and lastmod
                    # Parse URL entries
                    url_entries = re.findall(
                        r'<url>\s*<loc>([^<]+)</loc>(?:\s*<lastmod>([^<]*)</lastmod>)?',
                        content,
                        re.DOTALL
                    )

                    for loc, lastmod in url_entries:
                        loc = loc.strip()
                        all_urls.add(loc)
                        total_urls += 1
                        if lastmod:
                            all_lastmod[loc] = lastmod.strip()

                    # Also count URLs that might have different ordering
                    if not url_entries:
                        loc_matches = re.findall(r'<loc>([^<]+)</loc>', content)
                        for loc in loc_matches:
                            loc = loc.strip()
                            all_urls.add(loc)
                            total_urls += 1

            except Exception as e:
                all_errors.append(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É {url}: {str(e)}")

        # Process all sitemap URLs
        for sitemap_url in sitemap_urls[:10]:  # Limit to 10 sitemaps from robots.txt
            await parse_sitemap(sitemap_url)

        sitemap_data = SitemapData(
            exists=exists,
            url=sitemap_urls[0] if sitemap_urls else "",
            urls_count=total_urls,
            errors=all_errors[:10],
            sitemap_count=sitemap_count,
        )

        return sitemap_data, all_urls, all_lastmod
