"""Content analyzer."""

from typing import Any, Dict, List

from ..config import settings
from ..models import AnalyzerResult, AuditIssue, PageData, SeverityLevel
from .base import BaseAnalyzer


class ContentAnalyzer(BaseAnalyzer):
    """Analyzer for page content (word count, thin content)."""

    name = "content"
    display_name = "–ö–æ–Ω—Ç–µ–Ω—Ç"
    description = "–î–æ—Å—Ç–∞—Ç–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å —è–∫—ñ—Å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É –≤–∞–∂–ª–∏–≤–∞ –¥–ª—è —Ä–∞–Ω–∂—É–≤–∞–Ω–Ω—è —É –ø–æ—à—É–∫–æ–≤–∏—Ö —Å–∏—Å—Ç–µ–º–∞—Ö."
    icon = "üìÑ"
    theory = """<strong>–ö–æ–Ω—Ç–µ–Ω—Ç (—Ç–µ–∫—Å—Ç–æ–≤–∏–π –≤–º—ñ—Å—Ç)</strong> ‚Äî —Ü–µ –æ—Å–Ω–æ–≤–∞ SEO. –ü–æ—à—É–∫–æ–≤—ñ —Å–∏—Å—Ç–µ–º–∏ –∞–Ω–∞–ª—ñ–∑—É—é—Ç—å —Ç–µ–∫—Å—Ç –¥–ª—è —Ä–æ–∑—É–º—ñ–Ω–Ω—è —Ç–µ–º–∞—Ç–∏–∫–∏ —Ç–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏.

<strong>–ú—ñ–Ω—ñ–º–∞–ª—å–Ω—ñ –≤–∏–º–æ–≥–∏ –¥–æ –æ–±'—î–º—É:</strong>
‚Ä¢ <strong>–ö–∞—Ä—Ç–∫–∏ —Ç–æ–≤–∞—Ä—ñ–≤:</strong> 300-500 —Å–∏–º–≤–æ–ª—ñ–≤ (–±–µ–∑ –ø—Ä–æ–±—ñ–ª—ñ–≤) ‚Äî –æ–ø–∏—Å, —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏, –ø–µ—Ä–µ–≤–∞–≥–∏
‚Ä¢ <strong>–ö–∞—Ç–µ–≥–æ—Ä—ñ—ó/—Ä–æ–∑–¥—ñ–ª–∏:</strong> 1500-2500 —Å–∏–º–≤–æ–ª—ñ–≤ ‚Äî SEO-—Ç–µ–∫—Å—Ç –∑ –∫–ª—é—á–æ–≤–∏–º–∏ —Å–ª–æ–≤–∞–º–∏
‚Ä¢ <strong>–°—Ç–∞—Ç—Ç—ñ –±–ª–æ–≥—É:</strong> 3000-5000+ —Å–∏–º–≤–æ–ª—ñ–≤ ‚Äî –≥–ª–∏–±–æ–∫–µ —Ä–æ–∑–∫—Ä–∏—Ç—Ç—è —Ç–µ–º–∏
‚Ä¢ <strong>–ì–æ–ª–æ–≤–Ω–∞ —Å—Ç–æ—Ä—ñ–Ω–∫–∞:</strong> 1000-2000 —Å–∏–º–≤–æ–ª—ñ–≤ ‚Äî –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü—ñ—è –∫–æ–º–ø–∞–Ω—ñ—ó/–ø—Ä–æ–¥—É–∫—Ç—É

<strong>–¢–æ–Ω–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç (Thin Content):</strong>
–°—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –º—ñ–Ω—ñ–º–∞–ª—å–Ω–∏–º –∞–±–æ –Ω–µ—É–Ω—ñ–∫–∞–ª—å–Ω–∏–º —Ç–µ–∫—Å—Ç–æ–º –º–æ–∂—É—Ç—å:
‚Ä¢ –ù–µ –ø–æ—Ç—Ä–∞–ø–∏—Ç–∏ –≤ —ñ–Ω–¥–µ–∫—Å Google
‚Ä¢ –ü–æ—Ç—Ä–∞–ø–∏—Ç–∏ –ø—ñ–¥ —Ñ—ñ–ª—å—Ç—Ä Panda
‚Ä¢ –ü–æ–≥—ñ—Ä—à–∏—Ç–∏ –∑–∞–≥–∞–ª—å–Ω—É —è–∫—ñ—Å—Ç—å —Å–∞–π—Ç—É

<strong>–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó:</strong>
‚Ä¢ –ü–∏—à—ñ—Ç—å —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –¥–ª—è –∫–æ–∂–Ω–æ—ó —Å—Ç–æ—Ä—ñ–Ω–∫–∏
‚Ä¢ –í—ñ–¥–ø–æ–≤—ñ–¥–∞–π—Ç–µ –Ω–∞ –∑–∞–ø–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –ø–æ–≤–Ω–æ —Ç–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ
‚Ä¢ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É–π—Ç–µ –∫–ª—é—á–æ–≤—ñ —Å–ª–æ–≤–∞ –ø—Ä–∏—Ä–æ–¥–Ω–æ (–±–µ–∑ –ø–µ—Ä–µ—Å–ø–∞–º—É)
‚Ä¢ –î–æ–¥–∞–≤–∞–π—Ç–µ –∫–æ—Ä–∏—Å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é: —Ç–∞–±–ª–∏—Ü—ñ, —Å–ø–∏—Å–∫–∏, FAQ
‚Ä¢ –†–µ–≥—É–ª—è—Ä–Ω–æ –æ–Ω–æ–≤–ª—é–π—Ç–µ –∑–∞—Å—Ç–∞—Ä—ñ–ª–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç

<strong>–©–æ –ù–ï –≤–≤–∞–∂–∞—î—Ç—å—Å—è –∫–æ—Ä–∏—Å–Ω–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º:</strong>
‚Ä¢ –¢–µ–∫—Å—Ç, –ø—Ä–∏—Ö–æ–≤–∞–Ω–∏–π CSS (display:none)
‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ –∑–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π —Ç–µ–∫—Å—Ç
‚Ä¢ –°–∫–æ–ø—ñ–π–æ–≤–∞–Ω–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∑ —ñ–Ω—à–∏—Ö —Å–∞–π—Ç—ñ–≤"""

    async def analyze(
        self,
        pages: Dict[str, PageData],
        base_url: str,
        **kwargs: Any
    ) -> AnalyzerResult:
        issues: List[AuditIssue] = []
        tables: List[Dict[str, Any]] = []

        # Analyze content on each page
        thin_content = []
        empty_pages = []
        word_counts = []

        for url, page in pages.items():
            if page.status_code != 200:
                continue

            word_count = page.word_count
            word_counts.append((url, word_count))

            if word_count == 0:
                empty_pages.append(url)
            elif word_count < settings.MIN_CONTENT_WORDS:
                thin_content.append((url, word_count))

        # Sort by word count (ascending) for thin content
        thin_content.sort(key=lambda x: x[1])
        word_counts.sort(key=lambda x: x[1])

        # Create issues
        if empty_pages:
            issues.append(self.create_issue(
                category="empty_pages",
                severity=SeverityLevel.ERROR,
                message=f"–ü–æ—Ä–æ–∂–Ω—ñ —Å—Ç–æ—Ä—ñ–Ω–∫–∏: {len(empty_pages)} —à—Ç.",
                details="–°—Ç–æ—Ä—ñ–Ω–∫–∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É –Ω–µ –Ω–µ—Å—É—Ç—å —Ü—ñ–Ω–Ω–æ—Å—Ç—ñ –¥–ª—è –ø–æ—à—É–∫–æ–≤–∏—Ö —Å–∏—Å—Ç–µ–º.",
                affected_urls=empty_pages[:20],
                recommendation="–î–æ–¥–∞–π—Ç–µ —É–Ω—ñ–∫–∞–ª—å–Ω–∏–π —Ç–µ–∫—Å—Ç–æ–≤–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –∞–±–æ –Ω–∞–ª–∞—à—Ç—É–π—Ç–µ noindex –¥–ª—è —Ü–∏—Ö —Å—Ç–æ—Ä—ñ–Ω–æ–∫.",
                count=len(empty_pages),
            ))

        if thin_content:
            issues.append(self.create_issue(
                category="thin_content",
                severity=SeverityLevel.WARNING,
                message=f"–°—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –º–∞–ª–æ—é –∫—ñ–ª—å–∫—ñ—Å—Ç—é –∫–æ–Ω—Ç–µ–Ω—Ç—É: {len(thin_content)} —à—Ç.",
                details=f"–°—Ç–æ—Ä—ñ–Ω–∫–∏ –º—ñ—Å—Ç—è—Ç—å –º–µ–Ω—à–µ {settings.MIN_CONTENT_WORDS} —Å–ª—ñ–≤. –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä—ñ–π —Ç–∞ —Å—Ç–∞—Ç–µ–π —Ä–µ–∫–æ–º–µ–Ω–¥—É—î—Ç—å—Å—è –±—ñ–ª—å—à–µ —Ç–µ–∫—Å—Ç—É.",
                affected_urls=[url for url, _ in thin_content[:20]],
                recommendation="–†–æ–∑—à–∏—Ä—Ç–µ –∫–æ–Ω—Ç–µ–Ω—Ç, –¥–æ–¥–∞–≤—à–∏ –∫–æ—Ä–∏—Å–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –¥–ª—è –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤.",
                count=len(thin_content),
            ))

        # Create table with thin content pages
        table_data = []

        for url in empty_pages[:5]:
            table_data.append({
                "URL": url[:70] + "..." if len(url) > 70 else url,
                "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤": 0,
                "–°—Ç–∞—Ç—É—Å": "–ü–æ—Ä–æ–∂–Ω—è",
            })

        for url, count in thin_content[:15]:
            table_data.append({
                "URL": url[:70] + "..." if len(url) > 70 else url,
                "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤": count,
                "–°—Ç–∞—Ç—É—Å": "–ú–∞–ª–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É",
            })

        if table_data:
            tables.append({
                "title": "–°—Ç–æ—Ä—ñ–Ω–∫–∏ –∑ –Ω–µ–¥–æ—Å—Ç–∞—Ç–Ω—ñ–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º",
                "headers": ["URL", "–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤", "–°—Ç–∞—Ç—É—Å"],
                "rows": table_data,
            })

        # Calculate statistics
        total_pages = len(word_counts)
        if word_counts:
            total_words = sum(wc for _, wc in word_counts)
            avg_words = total_words // total_pages if total_pages > 0 else 0
            min_words = min(wc for _, wc in word_counts)
            max_words = max(wc for _, wc in word_counts)
        else:
            avg_words = min_words = max_words = 0

        # Summary
        ok_pages = total_pages - len(empty_pages) - len(thin_content)

        if not issues:
            summary = f"–í—Å—ñ {total_pages} —Å—Ç–æ—Ä—ñ–Ω–æ–∫ –º–∞—é—Ç—å –¥–æ—Å—Ç–∞—Ç–Ω—å–æ –∫–æ–Ω—Ç–µ–Ω—Ç—É. –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤: {avg_words}"
        else:
            parts = []
            if empty_pages:
                parts.append(f"–ø–æ—Ä–æ–∂–Ω—ñ—Ö: {len(empty_pages)}")
            if thin_content:
                parts.append(f"–∑ –º–∞–ª–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º: {len(thin_content)}")
            summary = f"–ü—Ä–æ–±–ª–µ–º–∏ –∑ –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º: {', '.join(parts)}. –°–µ—Ä–µ–¥–Ω—è –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤: {avg_words}"

        severity = self._determine_overall_severity(issues)

        return self.create_result(
            severity=severity,
            summary=summary,
            issues=issues,
            data={
                "total_pages": total_pages,
                "empty_pages": len(empty_pages),
                "thin_content": len(thin_content),
                "ok_pages": ok_pages,
                "avg_words": avg_words,
                "min_words": min_words,
                "max_words": max_words,
                "min_required": settings.MIN_CONTENT_WORDS,
            },
            tables=tables,
        )
